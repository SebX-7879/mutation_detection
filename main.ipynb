{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUTATION DETECTION CHALLENGE\n",
    "\n",
    "### 0. Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path #########\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from trainer import TorchTrainer\n",
    "from trainer.utils import slide_level_train_step, slide_level_val_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path can also be read from a config file, etc.\n",
    "OPENSLIDE_PATH = r\"C:\\Users\\Sébastien Mandela\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openslide-win64-20230414\\openslide-win64-20230414\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sébastien Mandela\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openslide-win64-20230414\\openslide-win64-20230414\\bin\n"
     ]
    }
   ],
   "source": [
    "print(OPENSLIDE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if hasattr(os, 'add_dll_directory'):\n",
    "    # Windows\n",
    "    with os.add_dll_directory(OPENSLIDE_PATH):\n",
    "        import openslide\n",
    "else:\n",
    "    import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.features import pad_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Sébastien Mandela\\\\OneDrive\\\\Documents\\\\École Polytechnique\\\\4A\\\\Owkin_challenge\\\\mutation_detection', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\\\python311.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0', '', 'C:\\\\Users\\\\Sébastien Mandela\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\Sébastien Mandela\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Sébastien Mandela\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Sébastien Mandela\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\Pythonwin', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\Sébastien Mandela\\\\OneDrive\\\\Documents\\\\École Polytechnique\\\\4A\\\\Owkin_challenge\\\\mutation_detection', 'C:\\\\Users\\\\Sébastien Mandela\\\\OneDrive\\\\Documents\\\\École Polytechnique\\\\4A\\\\Owkin_challenge\\\\mutation_detection']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "working_directory = Path(\".\").resolve() #########\n",
    "sys.path.append(str(working_directory))\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets.core import SlideFeaturesDataset\n",
    "from models.chowder import Chowder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the precomputed features\n",
    "train_features_dir = working_directory / \"data\" / \"train_input\" / \"moco_features\"\n",
    "test_features_dir = working_directory / \"data\" / \"test_input\" / \"moco_features\"\n",
    "\n",
    "## List of all the files in each directory\n",
    "train_features_path_all = list(train_features_dir.glob(\"*.npy\"))\n",
    "test_features_path_all = list(test_features_dir.glob(\"*.npy\"))\n",
    "\n",
    "## 2. Load the metadata\n",
    "train_metadata_df = pd.read_csv(working_directory / \"data\" / \"supplementary_data\" / \"train_metadata.csv\")\n",
    "test_metadata_df = pd.read_csv(working_directory / \"data\" / \"supplementary_data\" / \"test_metadata.csv\")\n",
    "\n",
    "## 3. Load the traning labels\n",
    "y_train = pd.read_csv(working_directory / \"data\" / \"train_output.csv\")\n",
    "## And concatenate the labels to the metadata\n",
    "train_metadata_df = pd.merge(train_metadata_df, y_train, on=\"Sample ID\")\n",
    "\n",
    "y_train = train_metadata_df[\"Target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344, 4)\n",
      "(149, 3)\n",
      "    Sample ID Patient ID Center ID  Target\n",
      "0  ID_001.npy      P_001       C_1       0\n",
      "1  ID_002.npy      P_002       C_2       1\n",
      "2  ID_005.npy      P_005       C_5       0\n",
      "3  ID_006.npy      P_006       C_5       0\n",
      "4  ID_007.npy      P_007       C_2       1\n",
      "    Sample ID Patient ID Center ID\n",
      "0  ID_003.npy      P_003       C_3\n",
      "1  ID_004.npy      P_004       C_4\n",
      "2  ID_008.npy      P_008       C_4\n",
      "3  ID_009.npy      P_009       C_4\n",
      "4  ID_010.npy      P_010       C_3\n"
     ]
    }
   ],
   "source": [
    "## Peek at the metadata\n",
    "print(train_metadata_df.shape) ## (344, 4)\n",
    "print(test_metadata_df.shape) ## (149, 3)\n",
    "\n",
    "print(train_metadata_df.head())\n",
    "print(test_metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Train dataset\n",
    "train_dataset = SlideFeaturesDataset(\n",
    "    features = train_features_path_all,\n",
    "    labels = y_train,\n",
    "    n_tiles=1000,\n",
    "    shuffle=True,\n",
    "    transform=None\n",
    ")\n",
    "## 5. Test dataset\n",
    "test_dataset = SlideFeaturesDataset(\n",
    "    features = test_features_path_all,\n",
    "    labels = [0]*len(test_features_path_all), ## Dummy labels, won't be used\n",
    "    n_tiles=1000,\n",
    "    shuffle=False,\n",
    "    transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (x, y) in train_dataset:\n",
    "#     print(x.shape, y)\n",
    "## All the WSI have 1000 tiles !\n",
    "\n",
    "# for (x, y) in test_dataset:\n",
    "#     print(x.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarly to Phikon_SSL_for_histopathology notebook\n",
    "train_indices = np.arange(len(train_dataset))\n",
    "train_labels = train_dataset.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chowder = Chowder(\n",
    "    in_features=2048,\n",
    "    out_features=1,\n",
    "    n_top=5,\n",
    "    n_bottom=5,\n",
    "    mlp_hidden=[200,100],\n",
    "    mlp_activation = torch.nn.Sigmoid(),\n",
    "    bias=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24450 || all params: 24450 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "## Parameters count\n",
    "def print_trainable_parameters(model: torch.nn) -> None:\n",
    "    \"\"\"Print number of trainable parameters.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param}\"\n",
    "        f\" || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "    \n",
    "print_trainable_parameters(chowder) ## 24,450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We define the loss function, optimizer and metrics for the training\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = torch.optim.Adam              # Adam optimizer\n",
    "metrics = {\"auc\": roc_auc_score}                    # AUC will be the tracking metric\n",
    "\n",
    "collator = pad_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run a 5-fold cross-validation with 1 repeat (you can tweak these parameters)\n",
    "n_repeats = 1\n",
    "n_folds = 5\n",
    "train_metrics, val_metrics = [], []\n",
    "test_logits = []\n",
    "\n",
    "cv_start_time = datetime.now()\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "    print(f\"Running cross-validation #{repeat+1}\")\n",
    "    # We stratify with respect to the training labels\n",
    "    cv_skfold = StratifiedKFold(\n",
    "        n_splits=n_folds,\n",
    "        shuffle=True,\n",
    "        random_state=repeat,\n",
    "    )\n",
    "    cv_splits = cv_skfold.split(train_indices, y = train_labels)\n",
    "\n",
    "    # 1 training fold approximately takes 25 seconds\n",
    "    for i, (train_indices_, val_indices_) in enumerate(cv_splits):\n",
    "        fold_start_time = datetime.now()\n",
    "        trainer = TorchTrainer(\n",
    "            model=deepcopy(chowder),\n",
    "            criterion=criterion,\n",
    "            metrics=metrics,\n",
    "            batch_size=16,                           # you can tweak this\n",
    "            num_epochs=15,                           # you can tweak this\n",
    "            learning_rate=1e-3,                      # you can tweak this\n",
    "            weight_decay=0.0,                        # you can tweak this\n",
    "            device=\"cuda:0\",\n",
    "            optimizer=deepcopy(optimizer),\n",
    "            train_step=slide_level_train_step,\n",
    "            val_step=slide_level_val_step,\n",
    "            collator=pad_collate_fn,\n",
    "        )\n",
    "\n",
    "        print(f\"Running cross-validation on split #{i+1}\")\n",
    "        train_dataset_ = torch.utils.data.Subset(\n",
    "            train_dataset, indices=train_indices_\n",
    "        )\n",
    "        val_dataset_ = torch.utils.data.Subset(\n",
    "            train_dataset, indices=val_indices_\n",
    "        )\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            # Training step for the given number of epochs\n",
    "            local_train_metrics, local_val_metrics = trainer.train(\n",
    "                train_dataset_, val_dataset_\n",
    "            )\n",
    "            # Predictions on test (logits, sigmoid(logits) = probability)\n",
    "            local_test_logits = trainer.predict(test_dataset)[1]\n",
    "\n",
    "        train_metrics.append(local_train_metrics)\n",
    "        val_metrics.append(local_val_metrics)\n",
    "        test_logits.append(local_test_logits)\n",
    "        fold_end_time = datetime.now()\n",
    "        fold_running_time = fold_end_time - fold_start_time\n",
    "        print(\"\\n-----------------------------Finished in {}---------------------------------------\\n\".format(fold_running_time))\n",
    "    clear_output()\n",
    "cv_end_time = datetime.now()\n",
    "cv_running_time = cv_end_time - cv_start_time\n",
    "print(\"\\nFinished cross-validation in {}\".format(cv_running_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
