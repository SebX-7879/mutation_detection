{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUTATION DETECTION CHALLENGE\n",
    "\n",
    "### 0. Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import sys\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path can also be read from a config file, etc.\n",
    "# OPENSLIDE_PATH = r\"C:\\Users\\SÃ©bastien Mandela\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openslide-win64-20230414\\openslide-win64-20230414\\bin\"\n",
    "# print(OPENSLIDE_PATH)\n",
    "# import os\n",
    "# if hasattr(os, 'add_dll_directory'):\n",
    "#     # Windows\n",
    "#     with os.add_dll_directory(OPENSLIDE_PATH):\n",
    "#         import openslide\n",
    "# else:\n",
    "#     import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users/eleves-b/2021/sebastien.mandela-yvon/Desktop/mutation_detection', '/usr/lib64/python39.zip', '/usr/lib64/python3.9', '/usr/lib64/python3.9/lib-dynload', '', '/users/eleves-b/2021/sebastien.mandela-yvon/Desktop/mutation_detection/.venv/lib64/python3.9/site-packages', '/users/eleves-b/2021/sebastien.mandela-yvon/Desktop/mutation_detection/.venv/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "working_directory = Path(\".\").resolve() #########\n",
    "#sys.path.append(str(working_directory))\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try rerunning this cell in case of module import error\n",
    "from utils.features import pad_collate_fn\n",
    "from utils.functional import sigmoid, softmax\n",
    "from datasets.core import SlideFeaturesDataset\n",
    "from models.chowder import Chowder\n",
    "from trainer import TorchTrainer\n",
    "from trainer.utils import slide_level_train_step, slide_level_val_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the precomputed features\n",
    "train_features_dir = working_directory / \"data\" / \"train_input\" / \"moco_features\"\n",
    "test_features_dir = working_directory / \"data\" / \"test_input\" / \"moco_features\"\n",
    "\n",
    "## List of all the files in each directory\n",
    "train_features_path_all = list(train_features_dir.glob(\"*.npy\"))\n",
    "test_features_path_all = list(test_features_dir.glob(\"*.npy\"))\n",
    "\n",
    "## 2. Load the metadata\n",
    "train_metadata_df = pd.read_csv(working_directory / \"data\" / \"supplementary_data\" / \"train_metadata.csv\")\n",
    "test_metadata_df = pd.read_csv(working_directory / \"data\" / \"supplementary_data\" / \"test_metadata.csv\")\n",
    "\n",
    "## 3. Load the traning labels\n",
    "y_train = pd.read_csv(working_directory / \"data\" / \"train_output.csv\")\n",
    "## And concatenate the labels to the metadata\n",
    "train_metadata_df = pd.merge(train_metadata_df, y_train, on=\"Sample ID\")\n",
    "\n",
    "y_train = train_metadata_df[\"Target\"].values.astype(np.float32) ## float32 required for BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344, 4)\n",
      "(149, 3)\n",
      "    Sample ID Patient ID Center ID  Target\n",
      "0  ID_001.npy      P_001       C_1       0\n",
      "1  ID_002.npy      P_002       C_2       1\n",
      "2  ID_005.npy      P_005       C_5       0\n",
      "3  ID_006.npy      P_006       C_5       0\n",
      "4  ID_007.npy      P_007       C_2       1\n",
      "    Sample ID Patient ID Center ID\n",
      "0  ID_003.npy      P_003       C_3\n",
      "1  ID_004.npy      P_004       C_4\n",
      "2  ID_008.npy      P_008       C_4\n",
      "3  ID_009.npy      P_009       C_4\n",
      "4  ID_010.npy      P_010       C_3\n"
     ]
    }
   ],
   "source": [
    "## Peek at the metadata\n",
    "print(train_metadata_df.shape) ## (344, 4)\n",
    "print(test_metadata_df.shape) ## (149, 3)\n",
    "\n",
    "print(train_metadata_df.head())\n",
    "print(test_metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Train dataset\n",
    "train_dataset = SlideFeaturesDataset(\n",
    "    features = train_features_path_all,\n",
    "    labels = y_train,\n",
    "    n_tiles=1000,\n",
    "    shuffle=True,\n",
    "    transform=None\n",
    ")\n",
    "## 5. Test dataset\n",
    "test_dataset = SlideFeaturesDataset(\n",
    "    features = test_features_path_all,\n",
    "    labels = np.zeros(len(test_features_path_all), dtype=np.float32), ## Dummy labels, won't be used\n",
    "    n_tiles=1000,\n",
    "    shuffle=False,\n",
    "    transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (x, y) in train_dataset:\n",
    "#     print(x.shape, y)\n",
    "# # All the WSI have 1000 tiles !\n",
    "\n",
    "# for (x, y) in test_dataset:\n",
    "#     print(x.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarly to Phikon_SSL_for_histopathology notebook\n",
    "train_indices = np.arange(len(train_dataset))\n",
    "train_labels = train_dataset.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chowder = Chowder(\n",
    "    in_features=2048,\n",
    "    out_features=1,\n",
    "    n_top=5,\n",
    "    n_bottom=5,\n",
    "    mlp_hidden=[200,100],\n",
    "    mlp_activation = torch.nn.Sigmoid(),\n",
    "    bias=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24450 || all params: 24450 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "## Parameters count\n",
    "def print_trainable_parameters(model: torch.nn) -> None:\n",
    "    \"\"\"Print number of trainable parameters.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param}\"\n",
    "        f\" || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "    \n",
    "print_trainable_parameters(chowder) ## 24,450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We define the loss function, optimizer and metrics for the training\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = torch.optim.Adam              # Adam optimizer\n",
    "metrics = {\"auc\": roc_auc_score}                    # AUC will be the tracking metric\n",
    "\n",
    "collator = pad_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model \n",
    "chowder_copy = deepcopy(chowder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished cross-validation in 0:02:34.518861\n"
     ]
    }
   ],
   "source": [
    "# We run a 5-fold cross-validation with 1 repeat (you can tweak these parameters)\n",
    "n_repeats = 1\n",
    "n_folds = 5\n",
    "train_metrics, val_metrics = [], []\n",
    "test_logits = []\n",
    "\n",
    "cv_start_time = datetime.now()\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "    print(f\"Running cross-validation #{repeat+1}\")\n",
    "    # We stratify with respect to the training labels\n",
    "    cv_skfold = StratifiedKFold(\n",
    "        n_splits=n_folds,\n",
    "        shuffle=True,\n",
    "        random_state=repeat,\n",
    "    )\n",
    "    cv_splits = cv_skfold.split(train_indices, y = train_labels)\n",
    "\n",
    "    # 1 training fold approximately takes 25 seconds\n",
    "    for i, (train_indices_, val_indices_) in enumerate(cv_splits):\n",
    "        fold_start_time = datetime.now()\n",
    "        trainer = TorchTrainer(\n",
    "            model=deepcopy(chowder),\n",
    "            criterion=criterion,\n",
    "            metrics=metrics,\n",
    "            batch_size=8,                           # you can tweak this\n",
    "            num_epochs=15,                           # you can tweak this\n",
    "            learning_rate=1e-3,                      # you can tweak this\n",
    "            weight_decay=0.0,                        # you can tweak this\n",
    "            device=\"cuda:0\",\n",
    "            optimizer=deepcopy(optimizer),\n",
    "            train_step=slide_level_train_step,\n",
    "            val_step=slide_level_val_step,\n",
    "            collator=pad_collate_fn,\n",
    "        )\n",
    "\n",
    "        print(f\"Running cross-validation on split #{i+1}\")\n",
    "        train_dataset_ = torch.utils.data.Subset(\n",
    "            train_dataset, indices=train_indices_\n",
    "        )\n",
    "        val_dataset_ = torch.utils.data.Subset(\n",
    "            train_dataset, indices=val_indices_\n",
    "        )\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            # Training step for the given number of epochs\n",
    "            local_train_metrics, local_val_metrics = trainer.train(\n",
    "                train_dataset_, val_dataset_\n",
    "            )\n",
    "            # Predictions on test (logits, sigmoid(logits) = probability)\n",
    "            local_test_logits = trainer.predict(test_dataset)[1]\n",
    "\n",
    "        train_metrics.append(local_train_metrics)\n",
    "        val_metrics.append(local_val_metrics)\n",
    "        test_logits.append(local_test_logits)\n",
    "        fold_end_time = datetime.now()\n",
    "        fold_running_time = fold_end_time - fold_start_time\n",
    "        print(\"\\n-----------------------------Finished in {}---------------------------------------\\n\".format(fold_running_time))\n",
    "    clear_output()\n",
    "cv_end_time = datetime.now()\n",
    "cv_running_time = cv_end_time - cv_start_time\n",
    "print(\"\\nFinished cross-validation in {}\".format(cv_running_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Plot the \n",
    "len(train_metrics), len(val_metrics), len(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149,)\n",
      "0.015914395 0.72321\n"
     ]
    }
   ],
   "source": [
    "test_probas = [sigmoid(logits) for logits in test_logits]\n",
    "test_probas = np.array(test_probas).mean(axis=0).squeeze()\n",
    "print(test_probas.shape)\n",
    "print(min(test_probas), max(test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metadata_df[\"Sample ID\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(\n",
    "    {\"Sample ID\": test_metadata_df[\"Sample ID\"].values, \"Target\": test_probas}\n",
    ").sort_values(\n",
    "    \"Sample ID\"\n",
    ")  # extra step to sort the sample IDs\n",
    "\n",
    "# sanity checks\n",
    "assert all(submission[\"Target\"].between(0, 1)), \"`Target` values must be in [0, 1]\"\n",
    "assert submission.shape == (149, 2), \"Your submission file must be of shape (149, 2)\"\n",
    "assert list(submission.columns) == [\n",
    "    \"Sample ID\",\n",
    "    \"Target\",\n",
    "], \"Your submission file must have columns `Sample ID` and `Target`\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
